{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a **sample 10-minute speech** integrating **all** the requested elements—**scenario**, **data**, **methods** (VGG16, FC1 layer, histogram equalization, PCA, t-SNE, whitening), **results**, and a **conclusion**. Time allocations are approximate. Feel free to adjust details or wording to fit your style.\n",
    "\n",
    "---\n",
    "\n",
    "## **Slide 1: Title & Scenario (≈ 1 minute)**\n",
    "\n",
    "> **“Hello everyone. My name is ___, and today I’ll be presenting our approach to **detecting 5 known and 1 new surface defect** using features from VGG16 and a novel 2-cluster detection scheme.**  \n",
    ">\n",
    "> Here’s the scenario: In our manufacturing plant, a **camera** inspects metal surfaces. We know of **5** defects—like Crazing, Patches, etc. Recently, we discovered a **new** type of defect that our old system didn’t recognize. We want to detect that new class automatically.”*\n",
    "\n",
    "---\n",
    "\n",
    "## **Slide 2: Data & Preprocessing (≈ 1 minute)**\n",
    "\n",
    "> **“We used a subset of the NEU surface defect dataset. We took **1,500** images for 5 known classes. Each image is 200×200 in grayscale. We also have a new batch of **40** images containing 20 from a known class, 20 from an unknown.  \n",
    ">  \n",
    "> **Preprocessing**: We do *histogram equalization*, specifically **contrast-limited adaptive histogram**. This operation adjusts the intensity distribution so features become more visible—spreading out pixel intensities to enhance contrast. Then we **resize** everything to 224×224, which is the input size for VGG16.”*\n",
    "\n",
    "---\n",
    "\n",
    "## **Slide 3: VGG16 & fc1 Features (≈ 1–2 minutes)**\n",
    "\n",
    "> **“Next, we use **VGG16**, a convolutional neural network architecture that was a top performer on ImageNet classification. It has 13 convolution layers and 3 fully-connected layers at the end. The last fully-connected is called `fc3`, but we’re interested in the **fc1** layer, which is the first fully-connected layer.  \n",
    ">\n",
    "> By extracting the **fc1** outputs, we get a **4096-dimensional** feature vector that describes the high-level content of each image. This is a classic example of **transfer learning**: VGG16 was originally trained on ImageNet (millions of images, 1,000 classes), but we reuse its learned filters to encode our NEU defect images.”*\n",
    "\n",
    "---\n",
    "\n",
    "## **Slide 4: PCA with SVD=Full & Whitening (≈ 1–2 minutes)**\n",
    "\n",
    "> **“After extracting 4096-d features, we run **PCA**—principal component analysis. Internally, PCA can use different solvers; we chose **SVD with ‘full’** to handle the entire dataset. PCA finds the directions of greatest variance, letting us reduce from 4096 to, say, 50 dimensions. \n",
    ">\n",
    "> Why 50? We tested the cumulative variance and found ~70–74% coverage.  \n",
    ">  \n",
    "> We also used **whitening**: that means each principal component is scaled to unit variance. This helps treat all components equally. Without whitening, large-variance components might dominate the distance metrics. But with whitening, we often get better cluster separation—though sometimes t-SNE visuals can look slightly less intuitive because all directions get scaled.”*\n",
    "\n",
    "---\n",
    "\n",
    "## **Slide 5: t-SNE Overview (≈ 1 minute)**\n",
    "\n",
    "> **“At times, we want to visualize these high-dimensional features. We use **t-SNE**, or **t-distributed Stochastic Neighbor Embedding**. t-SNE is a nonlinear method that aims to preserve local neighborhoods, projecting high-D data down to 2D. We use it to create scatter plots for clusters.  \n",
    ">  \n",
    "> One caveat: t-SNE can be sensitive to perplexity and global distances. But it’s great for local cluster patterns. We often color the t-SNE scatter by known classes, or new vs. old clusters.”*\n",
    "\n",
    "---\n",
    "\n",
    "## **Slide 6: Base Data Clustering (≈ 1 minute)**\n",
    "\n",
    "> **“We then do **k-Means** on the base dataset’s 50D features, typically with 6 clusters to match the known classes (5 known plus maybe one extra). We get around 99% accuracy if we match them to the ground truth labels. That’s our ‘old system.’  \n",
    ">  \n",
    "> Now, a new batch arrives with 40 images, some from a known defect, some truly new. We handle that next.”*\n",
    "\n",
    "---\n",
    "\n",
    "## **Slide 7: Novelty Approach (2-Cluster Method) (≈ 1 minute)**\n",
    "\n",
    "> **“For the new 40 images, we again do VGG16 → fc1 (4096) → PCA → 50D. Then we run **k=2** clustering.  \n",
    ">  \n",
    "> - We label whichever cluster center is closer to **old center #0** (which we designate ‘Crazing’) as the known class.  \n",
    "> - The other cluster automatically becomes ‘new class.’  \n",
    ">\n",
    "> This is a straightforward approach when we suspect exactly one new defect type in the batch. For multiple unknown defects, we might adapt it further.”*\n",
    "\n",
    "---\n",
    "\n",
    "## **Slide 8: Results & Figures (≈ 2 minutes)**\n",
    "\n",
    "> **“**Here** we see a t-SNE plot of those 40 images in 50D, color-coded by their 2-cluster assignment. One cluster is labeled ‘Crazing’, the other ‘new class.’ We see roughly half the points in each cluster—mirroring our scenario. We can also embed a few images as insets to confirm they are visually distinct.  \n",
    ">\n",
    "> On the right, we see a simpler t-SNE with just **two** cluster centers. Because there are only two points, we set perplexity=1. Each center is placed in 2D; one is near the old center #0.”*\n",
    "\n",
    "---\n",
    "\n",
    "## **Slide 9: Conclusion & Future Work (≈ 1 minute)**\n",
    "\n",
    "> **“In summary, we built an approach that uses **VGG16** to get fc1 features, compresses them with **PCA** (with whitening), and does a **2-cluster** technique to discover a new defect. This achieves a robust novelty detection for that one new fault.  \n",
    ">  \n",
    "> In the future, we can expand on multiple unknown classes, refine distance thresholds, or incorporate semi-supervised labeling for repeated unknown appearances. Thank you.”*\n",
    "\n",
    "---\n",
    "\n",
    "## **Q&A and Demo (≈ final minute)**\n",
    "\n",
    "> - **Show** short code snippet: \n",
    "  ```python\n",
    "  # new batch -> fc1 -> pca(50D)\n",
    "  kmeans_2 = KMeans(n_clusters=2).fit(new_batch_50)\n",
    "  # cluster 0 => old center #0 => 'Crazing'\n",
    "  # cluster 1 => 'new class'\n",
    "  ```\n",
    "> - **Then** finalize with thanks and open up for questions.\n",
    "\n",
    "That’s your **10-minute** presentation. Good luck!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
